{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "os.environ['PYSPARK_PYTHON']=sys.executable\n",
    "os.environ['PYSPARK_DRIVEN_PYTHON']=sys.executable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit, when, col, avg , sum, max, min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a PySpark script that performs actions like count and show on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3| Carl|\n",
      "|  4| Dave|\n",
      "|  5|  Eve|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark= SparkSession.builder.getOrCreate()\n",
    "df=spark.createDataFrame([\n",
    "    (1,\"Alice\"),(2,\"Bob\"),(3,\"Carl\"),(4,\"Dave\"),(5,\"Eve\")\n",
    "],[\"id\",\"name\"])\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a PySpark script that applies transformations like filter and withColumn on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  3|Carl|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.filter(df[\"name\"].startswith(\"C\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 10|\n",
      "|  2|  Bob| 20|\n",
      "|  3| Carl| 30|\n",
      "|  4| Dave| 40|\n",
      "|  5|  Eve| 50|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.withColumn(\"age\",df[\"id\"]*10)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(age)|\n",
      "+--------+\n",
      "|     150|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    30.0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      50|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(sum(\"age\")).show()\n",
    "\n",
    "df2.select(avg(\"age\")).show()\n",
    "\n",
    "df2.select(max(\"age\")).show()\n",
    "\n",
    "df2.select(min(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Show how to write a PySpark DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+-------+\n",
      "|emp_name|  company|department|country|\n",
      "+--------+---------+----------+-------+\n",
      "|   James|Educative|      Engg|    USA|\n",
      "| Michael|   Google|      null|   Asia|\n",
      "|  Robert|     null| Marketing| Russia|\n",
      "|   Maria|  Netflix|   Finance|Ukraine|\n",
      "|    null|     null|      null|   null|\n",
      "+--------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('answer').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Educative\",\"Engg\",\"USA\"),\n",
    "    (\"Michael\",\"Google\",None,\"Asia\"),\n",
    "    (\"Robert\",None,\"Marketing\",\"Russia\"),\n",
    "    (\"Maria\",\"Netflix\",\"Finance\",\"Ukraine\"),\n",
    "    (None, None, None, None)\n",
    "  ]\n",
    "\n",
    "columns = [\"emp_name\",\"company\",\"department\",\"country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "csv_file_path = \"data123.csv\"\n",
    "df.write.option(\"header\", True).option(\"delimiter\",\",\").csv(csv_file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------+-------+\n",
      "|       emp_name|  company|department|country|\n",
      "+---------------+---------+----------+-------+\n",
      "|James Educative|Educative|      Engg|    USA|\n",
      "| Michael Google|   Google|        SD|   Asia|\n",
      "|  Robert Amazon|   Amazon| Marketing| Russia|\n",
      "|  Maria Netflix|  Netflix|   Finance|Ukraine|\n",
      "|   Bhai Hunters|  Hunters|   Shooter|  India|\n",
      "+---------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws\n",
    "data = [(\"James\",\"Educative\",\"Engg\",\"USA\"),\n",
    "    (\"Michael\",\"Google\",\"SD\",\"Asia\"),\n",
    "    (\"Robert\",\"Amazon\",\"Marketing\",\"Russia\"),\n",
    "    (\"Maria\",\"Netflix\",\"Finance\",\"Ukraine\"),\n",
    "    (\"Bhai\", \"Hunters\", \"Shooter\", \"India\")\n",
    "  ]\n",
    "\n",
    "columns = [\"emp_name\",\"company\",\"department\",\"country\"]\n",
    "# creating spark session\n",
    "spark = SparkSession.builder.appName(\"testing\").getOrCreate()\n",
    "# creating dataframe\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# transformation\n",
    "new_df = df.withColumn(\"emp_name\", concat_ws(' ', df.emp_name, df.company))\n",
    "# displaying dataframe\n",
    "new_df.show(truncate=True)\n",
    "# # write pyspark dataframe into multiple csv file\n",
    "new_df.write.option(\"delimiter\", \",\").mode(\"overwrite\").csv(\"new_employee\")\n",
    "# write pyspark dataframe into single csv file\n",
    "new_df.coalesce(1).write.option(\"delimiter\", \",\").mode(\"overwrite\").csv(\"new_employee\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Implement wordcount program in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|          a|    7|\n",
      "|        and|    4|\n",
      "|        SQL|    4|\n",
      "|        for|    3|\n",
      "|         is|    3|\n",
      "|       data|    3|\n",
      "|         of|    3|\n",
      "|      Spark|    3|\n",
      "|   queries.|    2|\n",
      "|      using|    2|\n",
      "|   provides|    2|\n",
      "|         in|    2|\n",
      "|programming|    2|\n",
      "|       with|    2|\n",
      "|distributed|    2|\n",
      "|         on|    2|\n",
      "|         It|    2|\n",
      "| structured|    2|\n",
      "|  interface|    2|\n",
      "|    working|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import required pckg\n",
    "from pyspark.sql.functions import explode,split,col\n",
    "\n",
    "df=spark.read.text(\"/home/lplab/Desktop/BDAL/33B/WEEK2/words\")\n",
    "#Apply Split, Explode and groupBy to get count()\n",
    "df_count=(\n",
    "  df.withColumn('word', explode(split(col('value'), ' ')))\n",
    "    .groupBy('word')\n",
    "    .count()\n",
    "    .sort('count', ascending=False)\n",
    ")\n",
    "\n",
    "#Display Output\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
