{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with K-means Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Exercises:\n",
    "1. Implement a PySpark script to handle any missing values and scale numerical features.\n",
    "2. Develop a PySpark script that uses the K-means algorithm to cluster data points.\n",
    "3. Develop a PySpark script that labels data points as anomalies based on their cluster assignments.\n",
    "4. Implement code to evaluate the effectiveness of the K-means clustering model in detecting anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file into a Spark df without inferring the schema and without considering the first row as header\n",
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.csv\")\n",
    "\n",
    "# define the column names for df\n",
    "column_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "# Convert the df without header to df with specified column names\n",
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Selecting the label column from df and grouping them by label then count the each unique value in label in  descending ordr\n",
    "\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
      "       6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
      "       1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
      "       1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
      "       2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
      "       5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
      "       2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
      "       6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
      "       5.81176268e-02, 5.74111170e-02]),\n",
      " array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
      "       3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
      "       3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
      "       2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
      "       2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
      "       5.0000000e-02, 3.3000000e-01])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "# Drop the columns \"protocol_type\", \"service\", and \"flag\" from the DataFrame\n",
    "# Keep only the numeric columns and cache the result for efficient reuse\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "# Create a VectorAssembler to assemble the feature vector from the numeric columns\n",
    "# Set the input columns to be all columns except the last one\n",
    "# Set the output column name as \"featureVector\"\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "\n",
    "# Create a KMeans model for clustering\n",
    "# Set the prediction column name as \"cluster\"\n",
    "# Set the features column name as \"featureVector\"\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "\n",
    "# Create a Pipeline to chain together the VectorAssembler and KMeans stages\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "\n",
    "# Fit the Pipeline model to the numeric_only DataFrame\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "\n",
    "# Get the KMeans model from the fitted Pipeline model\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "# print cluster centers\n",
    "pprint(kmeans_model.clusterCenters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|         normal.| 97278|\n",
      "|      0|           back.|  2203|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            spy.|     2|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the numeric_only DataFrame using the fitted Pipeline model\n",
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "\n",
    "# Select the \"cluster\" and \"label\" columns from the transformed DataFrame\n",
    "# Group the DataFrame by \"cluster\" and \"label\"\n",
    "# Count the occurrences of each unique combination of \"cluster\" and \"label\"\n",
    "# Order the result by \"cluster\" and the count of occurrences in descending order\n",
    "# Show the top 25 rows of the result\n",
    "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24257686995955.965\n",
      "15536711568008.854\n",
      "19215204119441.668\n",
      "595801400002.5406\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data, k):\n",
    "    # drop non-numeric columns\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    \n",
    "    # create a vctorassembler to assemble the feature vector from the numeric columns\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    \n",
    "    # create a KMeans model with a random seed, specified k\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    # create a pipeline to chain together the vctorassembler and KMeans stages\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    \n",
    "    # fit the pipeline model to the input df\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    \n",
    "    # get the KMeans model from the fitted pipeline model\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    \n",
    "    # get the training cost from KMeans modfel\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    \n",
    "    return training_cost\n",
    "\n",
    "# iterate through a range of values for k\n",
    "for k in list(range(20,100, 20)):\n",
    "    print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 22967263578826.312\n",
      "40 24051449729302.156\n",
      "60 4217842680460.3354\n",
      "80 11601400189802.18\n",
      "100 10914450899300.459\n"
     ]
    }
   ],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).\\\n",
    "        setPredictionCol(\"cluster\").\\\n",
    "        setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,101, 20)):\n",
    "    print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 88978.02401692684\n",
      "330 82721.35008461926\n",
      "360 77102.15117534516\n",
      "390 72495.57251859624\n",
      "420 66867.30733985612\n",
      "450 62903.26332547334\n",
      "480 58422.70941652663\n",
      "510 55438.201560245056\n",
      "540 53443.773659448816\n",
      "570 51363.73048668775\n",
      "600 49083.723416677196\n",
      "630 47455.25610319255\n",
      "660 45640.43960431267\n",
      "690 43419.99424008578\n",
      "720 42411.61144291233\n",
      "750 40927.25326573158\n",
      "780 38535.83910908871\n",
      "810 38056.34156665825\n",
      "840 36322.18692424943\n",
      "870 35696.49706375343\n",
      "900 33995.724754888855\n",
      "930 34043.309811621126\n",
      "960 32377.997125947786\n",
      "990 31089.609252061957\n",
      "1020 30680.39374282738\n",
      "1050 29470.31790763838\n",
      "1080 29397.136154894517\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def clustering_score_2(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    \n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    \n",
    "    # Create a standardscaler to scale the feature vector\n",
    "    # StandardScaler standardizes features by removing the mean and scaling to unit variance\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").\\\n",
    "        setWithStd(True).setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans().\\\n",
    "        setSeed(randint(100,100000)).\\\n",
    "        setK(k).\\\n",
    "        setMaxIter(40).\\\n",
    "        setTol(1.0e-5).\\\n",
    "        setPredictionCol(\"cluster\").\\\n",
    "        setFeaturesCol(\"scaledFeatureVector\")\n",
    "    \n",
    "    # Create a Pipeline to chain together the VectorAssembler, StandardScaler, and KMeans stages\n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(300, 1101, 30)):\n",
    "    print(k, clustering_score_2(numeric_only, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Define a function to create a pipeline for one-hot encoding of a categorical column\n",
    "def one_hot_pipeline(input_col):\n",
    "    # Create a StringIndexer to convert categorical values into indices\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col +\"_indexed\")\n",
    "    \n",
    "    # Create a OneHotEncoder to encode the indexed categorical values into a sparse vector representation\n",
    "    encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n",
    "    \n",
    "    # Create a Pipeline to chain together the StringIndexer and OneHotEncoder stages\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    \n",
    "    # Return the pipeline and the output column name of the OneHotEncoder stage\n",
    "    return pipeline, input_col + \"_vec\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# Define a function to calculate the entropy given counts of different values\n",
    "def entropy(counts):\n",
    "    # Filter out counts with zero values\n",
    "    values = [c for c in counts if (c > 0)]\n",
    "    \n",
    "    # Calculate the total count\n",
    "    n = sum(values)\n",
    "    \n",
    "    # Calculate the probability of each value\n",
    "    p = [v/n for v in values]\n",
    "    \n",
    "    # Calculate the entropy using the formula: -sum(p_i * log(p_i))\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])\n",
    "\n",
    "# The following code seems to be incomplete or out of place\n",
    "# It appears to be a part of the previous conversation related to one-hot encoding using StringIndexer and OneHotEncoder\n",
    "# I'm not sure where you intended this code to be placed or how it relates to the previous code snippet\n",
    "# Please provide additional context or clarification if needed\n",
    "indexer = StringIndexer().setInputCol(input_col).\\\n",
    "    setOutputCol(input_col +\"_indexed\")\n",
    "encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n",
    "pipeline = Pipeline().setStages([indexer, encoder])\n",
    "return pipeline, input_col + \"_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.557605039016584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Transform the DataFrame using the fitted Pipeline model and select relevant columns\n",
    "cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")\n",
    "\n",
    "# Group the DataFrame by \"cluster\" and \"label\", counting the occurrences of each combination and ordering by \"cluster\"\n",
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "\n",
    "# Define a window specification partitioned by \"cluster\"\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "\n",
    "# Calculate the probability of each label within each cluster\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "\n",
    "# Add a column \"p_col\" representing the calculated probabilities\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "# calculate the entropy for each cluster\n",
    "result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))).\\\n",
    "                                           alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "\n",
    "# calculate the weighted cluster entropy for each cluster\n",
    "result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n",
    "\n",
    "# aggregate the weighted cluster entropy across all clusters\n",
    "weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "\n",
    "# calculate the average weighted cluster entropy over all clusters and total records\n",
    "average_weighted_cluster_entropy = weighted_cluster_entropy_avg[0][0] / data.count()\n",
    "\n",
    "print(average_weighted_cluster_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "    # Apply one-hot encoding pipelines to \"protocol_type\", \"service\", and \"flag\" columns\n",
    "    (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "    (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "    (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "    \n",
    "    # Determine columns for vector assembly excluding \"label\", \"protocol_type\", \"service\", \"flag\"\n",
    "    assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "    \n",
    "    # Create a VectorAssembler to assemble the feature vector\n",
    "    assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n",
    "    \n",
    "    # Create a StandardScaler for feature scaling\n",
    "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)  \n",
    "    kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "    # Create a Pipeline to chain together the preprocessing and clustering stages\n",
    "    pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "    \n",
    "    # Fit the Pipeline model to the data\n",
    "    return pipeline.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_4(input_data, k):\n",
    "    # Fit the pipeline to the input data to obtain the pipeline model\n",
    "    pipeline_model = fit_pipeline_4(input_data, k)\n",
    "    \n",
    "    # Transform the input data using the fitted pipeline model and select relevant columns\n",
    "    cluster_label = pipeline_model.transform(input_data).select(\"cluster\", \"label\")\n",
    "    \n",
    "    # Group the DataFrame by \"cluster\" and \"label\", counting the occurrences of each combination and ordering by \"cluster\"\n",
    "    df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "    \n",
    "    # Define a window specification partitioned by \"cluster\"\n",
    "    w = Window.partitionBy(\"cluster\")\n",
    "    \n",
    "    # Calculate the probability of each label within each cluster\n",
    "    p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "    \n",
    "    # Add a column \"p_col\" representing the calculated probabilities\n",
    "    with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "    \n",
    "    # Calculate the entropy for each cluster\n",
    "    result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))).alias(\"entropy\"),\n",
    "                                               fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "    \n",
    "    # Calculate the weighted cluster entropy for each cluster\n",
    "    result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n",
    "    \n",
    "    # Aggregate the weighted cluster entropy across all clusters\n",
    "    weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "    \n",
    "    # Calculate the average weighted cluster entropy over all clusters and total records\n",
    "    return weighted_cluster_entropy_avg[0][0] / input_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|cluster|     label| count|\n",
      "+-------+----------+------+\n",
      "|      0|  neptune.| 82132|\n",
      "|      0|portsweep.|    15|\n",
      "|      1|  ipsweep.|     4|\n",
      "|      1|     nmap.|     1|\n",
      "|      1|   normal.|   337|\n",
      "|      1|      pod.|     5|\n",
      "|      1|portsweep.|     1|\n",
      "|      1|    smurf.|280787|\n",
      "|      2|  neptune.|   102|\n",
      "|      2|portsweep.|     1|\n",
      "|      2|    satan.|     1|\n",
      "|      3|  neptune.|    98|\n",
      "|      4|  neptune.|    79|\n",
      "|      4|portsweep.|     1|\n",
      "|      4|    satan.|     1|\n",
      "|      5|  neptune.|    98|\n",
      "|      5|portsweep.|     1|\n",
      "|      6|  neptune.|    92|\n",
      "|      6|    satan.|     1|\n",
      "|      7|  ipsweep.|     1|\n",
      "+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the data with k=180\n",
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "\n",
    "# Transform the data using the fitted pipeline model\n",
    "transformed_data = pipeline_model.transform(data)\n",
    "\n",
    "# Group the transformed data by \"cluster\" and \"label\", count the occurrences of each combination, and order by \"cluster\" and \"label\"\n",
    "count_by_cluster_label = transformed_data.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n",
    "\n",
    "# Show result\n",
    "count_by_cluster_label.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
