{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3af70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc4dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9919881",
   "metadata": {},
   "source": [
    "Q.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdaea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mean: float, var: float):\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        return img + torch.normal(self.mean, self.var, img.size())\n",
    "preprocess = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(45),\n",
    "    Gaussian(0,0.15),\n",
    "])\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, str=\"train\"):\n",
    "        self.imgs_path = \".\\\\data\\\\cats_and_dogs_filtered\\\\\"+ str + \"\\\\\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"\\\\\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"\\\\*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        self.class_map = {\"dogs\" : 0, \"cats\": 1}\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = PIL.Image.open(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        class_id = torch.tensor(class_id)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, class_id\n",
    "dataset = MyDataset(transform=preprocess,str = \"train\")\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8bc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code displays augmented images\n",
    "i=0\n",
    "for data in iter(dataloader):\n",
    "    img,_ = data\n",
    "    tI = T.ToPILImage()\n",
    "    img = tI(img.squeeze())\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    i = i + 1\n",
    "    if i == 3:\n",
    "        break\n",
    "#Extend the code to include training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236a17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32*14*14, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes (cat and dog)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 32*14*14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load the model, loss function, and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # L2 regularization\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a951419d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight, L2 Norm: 2.3536782264709473\n",
      "Layer: conv2.weight, L2 Norm: 3.2647111415863037\n",
      "Layer: fc1.weight, L2 Norm: 6.531321048736572\n",
      "Layer: fc2.weight, L2 Norm: 0.797450602054596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32*14*14, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes (cat and dog)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 32*14*14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, str=\"train\"):\n",
    "        self.imgs_path = \".\\\\data\\\\cats_and_dogs_filtered\\\\\" + str + \"\\\\\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"\\\\\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"\\\\*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        self.class_map = {\"dogs\": 0, \"cats\": 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = PIL.Image.open(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        class_id = torch.tensor(class_id)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, class_id\n",
    "\n",
    "# Load the dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(transform=transform, str=\"train\")\n",
    "test_dataset = MyDataset(transform=transform, str=\"test\")\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # L2 regularization\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = correct / total\n",
    "            print(f'Epoch {epoch+1}, Accuracy: {accuracy}')\n",
    "\n",
    "# Observe the impact of regularization on the weight parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        l2_norm = torch.norm(param, p=2)  # L2 norm of weights\n",
    "        print(f'Layer: {name}, L2 Norm: {l2_norm.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de116b9",
   "metadata": {},
   "source": [
    "Q.2a. L1 regularization using optimizer's weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c16be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight, L1 Norm: 40.86850357055664\n",
      "Layer: conv2.weight, L1 Norm: 195.38363647460938\n",
      "Layer: fc1.weight, L1 Norm: 5066.7001953125\n",
      "Layer: fc2.weight, L1 Norm: 11.316882133483887\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32*14*14, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes (cat and dog)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 32*14*14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Define the model, loss function, and optimizer with L1 regularization\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # L1 regularization\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = correct / total\n",
    "            print(f'Epoch {epoch+1}, Accuracy: {accuracy}')\n",
    "\n",
    "# Observe the impact of L1 regularization on the weight parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        l1_norm = torch.norm(param, p=1)  # L1 norm of weights\n",
    "        print(f'Layer: {name}, L1 Norm: {l1_norm.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a7baebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total L1 Norm: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the L1 norm to zero\n",
    "l1_norm = 0\n",
    "\n",
    "# Calculate the L1 norm of the weights\n",
    "for param in model.parameters():\n",
    "    if 'weight' in name:\n",
    "        l1_norm += torch.sum(torch.abs(param))\n",
    "\n",
    "print(f'Total L1 Norm: {l1_norm}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c52d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0361a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
